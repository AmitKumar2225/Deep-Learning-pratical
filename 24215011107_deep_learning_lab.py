# -*- coding: utf-8 -*-
"""24215011107_Deep_Learning_Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMVEVPqJaKY892fbmcT3GkCFmy0x6siP

Q1) Task: Given a sequence of alphabets (with some missing values), use an RNN model to predict the missing values in the sequence.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Step 2: Corpus
corpus = [
    "The sun rises in the east",
    "A bird flies over the forest",
    "The river runs through the hills",
    "Children laugh in the meadow",
    "The moon shines at night",
    "A dog runs in the yard",
    "The wind whispers in the trees",
    "People sing by the campfire",
    "The stars twinkle in the sky",
    "A cat sleeps on the mat",
    "The sun sets in the west",
    "A bird nests in the tree",
    "The river sparkles in the sun",
    "Children play by the stream"
]

# ðŸ“Œ Step 3: Tokenize the Text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
sequences = [tokenizer.texts_to_sequences([sent])[0] for sent in corpus]
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# ðŸ“Œ Step 4: Create Context-Target Pairs
X, y = [], []
window = 2

for seq in sequences:
    for i in range(window, len(seq) - window):
        context = seq[i-window:i] + seq[i+1:i+window+1]
        target = seq[i]
        X.append(context)
        y.append(target)

X = np.array(X)
y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)

# ðŸ“Œ Step 5: Define SimpleRNN Model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=64, input_length=4),
    SimpleRNN(128, activation='tanh'),
    Dense(vocab_size, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ðŸ“Œ Step 6: Train the Model
model.fit(X, y, epochs=100, batch_size=2, verbose=1)

# ðŸ”„ Step 9: Evaluate model performance on training data
# Convert predictions to class indices
y_pred_probs = model.predict(X, verbose=0)
y_pred = np.argmax(y_pred_probs, axis=1)

# Convert one-hot y to class indices
y_true = np.argmax(y, axis=1)

# âœ… Classification metrics (macro averages handle multiple classes)
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

print(f"âœ… Accuracy : {accuracy:.4f}")
print(f"âœ… Precision: {precision:.4f}")
print(f"âœ… Recall   : {recall:.4f}")
print(f"âœ… F1-score : {f1:.4f}")

# ðŸ“Œ Step 7: Predict Missing Word Function
reverse_word_index = {i: w for w, i in tokenizer.word_index.items()}

def predict_missing(context_text):
    # Convert context text to sequence
    sequence = tokenizer.texts_to_sequences([context_text])[0]

    # Pad the sequence to match input shape (length 4)
    padded = pad_sequences([sequence], maxlen=4, truncating='pre')

    # Make prediction
    prediction = model.predict(padded, verbose=0)

    # Get the index of the highest probability
    predicted_index = np.argmax(prediction)

    # Map the index back to the word
    predicted_word = reverse_word_index.get(predicted_index, "<unk>")

    return predicted_word

# ðŸ“Œ Step 8: Test Prediction
test_input = "the river in the"  # Expected missing word: "sparkles" or "runs"
predicted = predict_missing(test_input)
print(f"ðŸ§  Predicted missing word: {predicted}")

